{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXUWEDroXIR8TfkSl0IZxF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BuxinSu/ICML_Ranking/blob/main/Notebooks/Residual_variance_confidence_accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Google Drive**\n"
      ],
      "metadata": {
        "id": "IzvX_XX1CDtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg33FdknDKZV",
        "outputId": "b90248d5-24ee-44af-e6e0-c4fe0c2fd630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting**\n"
      ],
      "metadata": {
        "id": "Zve7gI2zCF_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.isotonic import isotonic_regression\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "import scipy.stats as stats\n",
        "import seaborn as sns\n",
        "# Set the text properties to use LaTeX fonts\n",
        "plt.rcParams['svg.fonttype'] = 'none'\n",
        "plt.rcParams['font.size'] = 30\n",
        "plt.rcParams['figure.figsize'] = (13,8)  # width, height in inches\n",
        "plt.rcParams['figure.dpi'] = 300  # dots per inch"
      ],
      "metadata": {
        "id": "7iOHQguoDLNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load CSV file into a pandas DataFrame**"
      ],
      "metadata": {
        "id": "Nqut4wZHB2c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data from a CSV file\n",
        "df_review = pd.read_csv('/content/drive/MyDrive/Research/Ranking ICML/Final_Datafile/review_daily_anon_2023_0422.csv')\n",
        "# Extract the first number from the 'rating' column based on the specific format of the rating content\n",
        "df_review['rating'] = df_review['rating'].str.extract(r'^(\\d+)', expand=False).astype(float)\n",
        "# Extract the first number from the 'confidence' column\n",
        "df_review['confidence'] = df_review['confidence'].str.extract(r'^(\\d+)', expand=False).astype(float)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(r'/content/drive/MyDrive/Research/Ranking_ICML/proxy_score.csv')\n",
        "df = df.drop_duplicates(['submission_idx', 'author_idx'])\n",
        "\n",
        "# check if all submissions have at least 2 reviews\n",
        "for submission in df['submission_id'].unique():\n",
        "  if len( df_review[df_review['submission_id'] == submission]['rating'].tolist() ) <= 1:\n",
        "      print(submission)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "final_submission_list = df['submission_id'].unique()\n",
        "authors = df['author_id'].unique()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_expect = pd.read_csv(r'/content/drive/MyDrive/Research/Ranking ICML/Final_Datafile/aggregated_results_anonymized.csv')\n",
        "df_expect = df_expect.dropna(subset=['submission_id_with_most_unexpected_scores'])\n",
        "\n",
        "\n",
        "\n",
        "# all authors providing unexpected submissions\n",
        "author_unexpect = df_expect['author_id'].unique()\n",
        "author_unexpect = [author for author in authors if author in author_unexpect]\n",
        "\n"
      ],
      "metadata": {
        "id": "gnB-kj5LDcqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Record Residual, Confidence and Variance**"
      ],
      "metadata": {
        "id": "TxSEFkBYB5Gn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubb_N9uNDHHa"
      },
      "outputs": [],
      "source": [
        "# Organize all the submissions by {author: [submission, rank, score]}.\n",
        "author_submission_rank_old = {}\n",
        "for author in authors:\n",
        "    author_submission_rank_old[author] = []\n",
        "    submissions = df[df['author_id'] == author]['submission_id'].tolist()\n",
        "    for i in range(len(submissions)):\n",
        "        rank = df[(df['submission_id'] == submissions[i]) & (df['author_id'] == author)]['rank'].tolist()[0]\n",
        "        ratings = df[(df['submission_id'] == submissions[i]) & (df['author_id'] == author)]['rating_0422_mean'].tolist()[0]\n",
        "        author_submission_rank_old[author].append((submissions[i], rank, ratings))\n",
        "\n",
        "\n",
        "\n",
        "# Sort submissions by rank; in case of ties, sort by score.\n",
        "def sort_submissions(author_submission_rank_old):\n",
        "    for author in author_submission_rank_old:\n",
        "        author_submission_rank_old[author].sort(key=lambda x: (x[1], -x[2]), reverse=False)\n",
        "    return author_submission_rank_old\n",
        "author_submission_rank_old = sort_submissions(author_submission_rank_old)\n",
        "\n",
        "\n",
        "\n",
        "# Compute isotonic scores for each author.\n",
        "author_submission_new = {}\n",
        "for author in author_submission_rank_old:\n",
        "    ir_rank = []\n",
        "    for i in range(len(author_submission_rank_old[author])):\n",
        "        r1 = author_submission_rank_old[author][i][2]\n",
        "        ir_rank.append(r1)\n",
        "    ir_rank = np.array(ir_rank)\n",
        "    ir_rank_pred =  isotonic_regression(ir_rank, sample_weight = None, y_min=0.0, y_max=10.0, increasing=False)\n",
        "\n",
        "    for i in range(len(author_submission_rank_old[author])):\n",
        "        author_submission_new[ (author, author_submission_rank_old[author][i][0]) ] = ir_rank_pred[i]\n",
        "\n",
        "\n",
        "\n",
        "# record average score\n",
        "author_submission_old = {}\n",
        "for author in author_submission_rank_old:\n",
        "    for i in range(len(author_submission_rank_old[author])):\n",
        "        author_submission_old[ (author, author_submission_rank_old[author][i][0]) ] = author_submission_rank_old[author][i][2]\n",
        "\n",
        "\n",
        "\n",
        "# record the residual\n",
        "residual = {}\n",
        "for author in author_unexpect:\n",
        "    submission_list = df[df['author_id'] == author ]['submission_id'].tolist()\n",
        "    for submission in submission_list:\n",
        "        residual[(author, submission)] = abs( author_submission_old[ (author, submission) ] - author_submission_new[ (author, submission) ] )\n",
        "\n",
        "\n",
        "\n",
        "# record confidence\n",
        "confidence = {}\n",
        "for author in author_unexpect:\n",
        "    submission_list = df[df['author_id'] == author ]['submission_id'].tolist()\n",
        "    for submission in submission_list:\n",
        "        confidences = df_review[df_review['submission_id'] == submission]['confidence'].tolist()\n",
        "        confidences = np.array(confidences)\n",
        "        confidence[(author, submission)] =  np.mean(confidences)\n",
        "\n",
        "\n",
        "\n",
        "# record variance\n",
        "variance = {}\n",
        "for author in author_unexpect:\n",
        "    submission_list = df[df['author_id'] == author ]['submission_id'].tolist()\n",
        "    for submission in submission_list:\n",
        "        ratings = df_review[df_review['submission_id'] == submission]['rating'].tolist()\n",
        "        ratings = np.array(ratings)\n",
        "        variance[(author, submission)] =  np.var(ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Count prediction accuracy**"
      ],
      "metadata": {
        "id": "Xu4RDXQLBBMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "residual_count = 0\n",
        "variance_count = 0\n",
        "confidence_count = 0\n",
        "total_authors = 0\n",
        "for author in author_unexpect:\n",
        "    submission_list = df[df['author_id'] == author ]['submission_id'].tolist()\n",
        "    unexpect_submission = df_expect[df_expect['author_id'] == author]['submission_id_with_most_unexpected_scores'].tolist()[0]\n",
        "    if unexpect_submission in submission_list:\n",
        "      total_authors += 1\n",
        "      if all( residual[(author, unexpect_submission)] >= residual[(author, submission)] for submission in submission_list ):\n",
        "          residual_count += 1\n",
        "      if all( variance[(author, unexpect_submission)] >= variance[(author, submission)] for submission in submission_list ):\n",
        "          variance_count += 1\n",
        "      if all( confidence[(author, unexpect_submission)] <= confidence[(author, submission)] for submission in submission_list ):\n",
        "          confidence_count += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"# authors with unexpected submissions:\", total_authors)\n",
        "print(\"# authors with unexpected submissions that have the largest mean residual:\", residual_count)\n",
        "print(\"# authors with unexpected submissions that have the largest variance:\", variance_count)\n",
        "print(\"# authors with unexpected submissions that have the least confidence:\", confidence_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRs2GoFqEyAG",
        "outputId": "b4bd8bd3-711e-4adb-ed0c-808dbb61ac49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# authors with unexpected submissions: 322\n",
            "# authors with unexpected submissions that have the largest mean residual: 254\n",
            "# authors with unexpected submissions that have the largest variance: 162\n",
            "# authors with unexpected submissions that have the least confidence: 136\n"
          ]
        }
      ]
    }
  ]
}